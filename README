setup:
1. git clone https://github.com/orenmn/qemu_with_GMBEOO
2. git clone https://github.com/orenmn/qemu_mem_tracer
3. Install python3.7 (according to
   https://askubuntu.com/questions/865554/how-do-i-install-python-3-6-using-apt-get/865569#865569):
	a. sudo add-apt-repository ppa:deadsnakes/ppa
	b. sudo apt-get update
	c. sudo apt-get install python3.7
4. sudo apt-get install expect
5. python3.7 qemu_mem_tracer/build.py qemu_with_GMBEOO_path
6. Either ask orenmn@gmail.com for a qemu image for memory_tracer, or create
   one yourself:
	a. Download ubuntu-18.04.1-live-server-amd64.iso
	   (https://www.ubuntu.com/download/server).
	b. Create an “empty” image. E.g.:
		- ./qemu-img create -f qcow2 oren_vm_disk.qcow2 25G
		  (See https://en.wikibooks.org/wiki/QEMU/Images#Creating_an_image and
		  https://wiki.qemu.org/Hosts/Linux#Getting_the_source_code)
	c. Install the Ubuntu Server on it:
		- ./x86_64-softmmu/qemu-system-x86_64 -m 2560 -hda ../oren_vm_disk2.qcow2 -cdrom ../ubuntu-18.04.1-desktop-amd64.iso -monitor stdio
			* Don’t worry about a “spectre v2 mitigation” message at the
			  beginning (it didn’t seem to ruin anything for me. The
			  startup just moves on).
			* It would probably take some time. An easy way to check
			  whether the guest is actually dead (and not just working
			  diligently but silently): use the qemu’s monitor command
			  `info registers` several times. If the registers change, then
			  you should probably just wait.
			* If you get “cannot set up guest memory 'pc.ram': Cannot
			  allocate memory“, then 2560MB is too much for your machine to
			  give qemu, so specify a smaller amount.
			* When the installation finishes, close qemu.
	d. Start qemu_with_GMBEOO. E.g.:
		- ./x86_64-softmmu/qemu-system-x86_64 -m 2560 -hda ../oren_vm_disk2.qcow2 -monitor stdio -serial pty
		- I highly recommend giving your qemu guest some time to completely
		  finish the startup process (even after logging in, i guess many
		  startup scripts are still running). Sounds obvious in retrospect, but
		  I didn’t do that at first, and used snapshots that were created right
		  after startup. When I started using snapshots that were taken only
		  after startup really completed, the speedup was significant.
		- Copy run_executables_from_serial from the host into the guest. E.g.
		  (inside the guest, 10.0.2.2 is the host):
			* scp orenmn@10.0.2.2:to_run_on_guest/run_executables_from_serial run_executables_from_serial
		- chmod 777 run_executables_from_serial
		- sudo chmod 666 /dev/ttyS0
		- ./run_executables_from_serial
			* run_executables_from_serial is waiting for input through the serial.
			  If it printed "Opened /dev/ttyS0.", proceed to the next step
			  without terminating run_executables_from_serial.
	e. Save a snapshot of the guest by executing the qemu monitor command
	   (monitor commands can be sent through the terminal you used to start
	   qemu_with_GMBEOO):
		- savevm ready_for_memory_tracer
	f. Close qemu.
7. Run qemu_mem_tracer/build.py qemu_with_GMBEOO_path --dont_compile_qemu --run_tests --guest_image_path GUEST_IMAGE_PATH --snapshot_name SNAPSHOT_NAME




usage: memory_tracer.py [-h]
                        (--workload_path_on_guest WORKLOAD_PATH_ON_GUEST | --workload_path_on_host WORKLOAD_PATH_ON_HOST)
                        (--analysis_tool_path ANALYSIS_TOOL_PATH | --trace_fifo_path TRACE_FIFO_PATH | --dont_trace | --dont_use_qemu)
                        [--trace_only_CPL3_code_GMBE]
                        [--log_of_GMBE_block_len LOG_OF_GMBE_BLOCK_LEN]
                        [--log_of_GMBE_tracing_ratio LOG_OF_GMBE_TRACING_RATIO]
                        [--timeout TIMEOUT | --dont_add_communications_with_host_to_workload]
                        [--print_trace_info] [--dont_exit_qemu_when_done]
                        [--verbose]
                        guest_image_path snapshot_name qemu_with_GMBEOO_path

Run a workload on the QEMU guest while writing optimized GMBE trace records to a FIFO.

(memory_tracer.py assumes you have already run build.py successfully.)

GMBE is short for guest_mem_before_exec. This is an event in upstream QEMU 3.0.0 that occurs on every attempt of the QEMU guest to access a virtual memory address.

We optimized QEMU's tracing code for the case in which only trace records of GMBE are gathered (we call it GMBE only optimization - GMBEOO, and so we gave our fork of QEMU the name qemu_with_GMBEOO).
When GMBEOO is enabled (in qemu_with_GMBEOO), a trace record is structured as follows:

struct GMBEOO_TraceRecord {
    uint8_t size_shift : 3; /* interpreted as "1 << size_shift" bytes */
    bool    sign_extend: 1; /* whether it is a sign-extended operation */
    uint8_t endianness : 1; /* 0: little, 1: big */
    bool    store      : 1; /* whether it is a store operation */
    uint8_t cpl        : 2;
    uint64_t unused2   : 56;
    uint64_t virt_addr : 64;
};

memory_tracer.py also prints the workload info (in case it isn't the empty string), and the tracing duration in miliseconds.
In case --analysis_tool_path is specified, memory_tracer.py also prints the output of the analysis tool.

Note that workload_runner can also be an ELF that includes the workload and the aforementioned prints.

If --analysis_tool_path is specified, the provided analysis tool must do the following:
1. Receive in argv[1] the path of the trace FIFO, but not open it for reading yet.2. Register a handler for the signal SIGUSR1 (e.g. by calling the `signal` syscall). The handler must:
    a. Print "-----begin analysis output-----".
    b. Print the output of the analysis tool.
    c. Print "-----end analysis output-----".
3. Print "Ready to analyze" when you wish the tracing to start.
4. Open the trace FIFO for read, and start reading trace records from it. Note that the reading from the FIFO should be as fast as possible. Otherwise, the FIFO's buffer would get full, and qemu_with_GMBEOO would start blocking when it tries to write to the FIFO. Soon, trace_buf would get full, and trace records of new GMBE events would be dropped.
(If any of the messages isn't printed, it will probably seem like memory_tracer.py is stuck.)

Note that some of the command line arguments might be irrelevant to you as a user of memory_tracer, but they exist because they are useful while developing memory_tracer.

positional arguments:
  guest_image_path      The path of the qcow2 file which is the image of the
                        guest.
  snapshot_name         The name of the snapshot saved by the monitor command
                        `savevm`, which was specially constructed for running
                        a workload with GMBE tracing.
  qemu_with_GMBEOO_path
                        The path of qemu_with_GMBEOO.

optional arguments:
  -h, --help            show this help message and exit
  --workload_path_on_guest WORKLOAD_PATH_ON_GUEST
                        The path of the workload on the guest.
  --workload_path_on_host WORKLOAD_PATH_ON_HOST
                        The path of the workload on the host. The file in that
                        path would be sent to the guest to run as the
                        workload.
  --analysis_tool_path ANALYSIS_TOOL_PATH
                        Path of an analysis tool that would start executing
                        before the tracing starts.
  --trace_fifo_path TRACE_FIFO_PATH
                        Path of the FIFO into which trace records will be
                        written. Note that as mentioned above, a scenario in
                        which the FIFO's buffer getting full is bad, and so it
                        is recommended to use a FIFO whose buffer is of size
                        `cat /proc/sys/fs/pipe-max-size`.
  --dont_trace          If specified, memory_tracer.py will run without
                        enabling the tracing feature of qemu_with_GMBEOO.
                        Therefore, it will not print the trace info (even if
                        --print_trace_info is specified). This is useful for
                        comparing the speed of qemu_with_GMBEOO with and
                        without tracing.
  --dont_use_qemu       If specified, memory_tracer.py will run the workload
                        on the host (i.e. native). Specifically, both
                        workload_runner and workload will be copied to a
                        temporary directory, and there workload_runner will be
                        executed. Please pass dummy non-empty strings as the
                        arguments guest_image_path, snapshot_name,
                        host_password and qemu_with_GMBEOO_path. As expected,
                        no trace info will be printed (even if
                        --print_trace_info is specified). Also, the analysis
                        tool will not be executed (even if
                        --analysis_tool_path is specified). This is useful for
                        comparing the speed of qemu_with_GMBEOO to running the
                        code natively. Note that This feature is somewhat
                        limited, as it only captures the prints by
                        workload_runner. (i.e. it would get stuck in case
                        workload_runner doesn't send all the expected messages
                        itself (e.g. "Ready to trace. Press enter to
                        continue").)
  --trace_only_CPL3_code_GMBE
                        If specified, qemu would only trace memory accesses by
                        CPL3 code. Otherwise, qemu would trace all accesses.
  --log_of_GMBE_block_len LOG_OF_GMBE_BLOCK_LEN
                        Log of the length of a GMBE_block, i.e. the number of
                        GMBE events in a GMBE_block. (It is used when
                        determining whether to trace a GMBE event.)
  --log_of_GMBE_tracing_ratio LOG_OF_GMBE_TRACING_RATIO
                        Log of the ratio between the number of blocks of GMBE
                        events we trace to the total number of blocks. E.g. if
                        GMBE_tracing_ratio is 16, we trace 1 block, then skip
                        15 blocks, then trace 1, then skip 15, and so on...
  --timeout TIMEOUT     If specified, the workload would be stopped when the
                        specified timeout elapses. Note that if you use
                        --dont_add_communications_with_host_to_workload, then
                        the timeout includes the communications with the host.
                        Otherwise, the timeout doesn't include the
                        communications.
  --dont_add_communications_with_host_to_workload
                        If specified, the workload would not be wrapped with
                        code that handles the required communications between
                        the guest and the host, i.e. the workload (given in
                        workload_path_on_host or workload_path_on_guest) must
                        do the following: 1. Print "-----begin workload
                        info-----". 2. Print runtime info of the workload.
                        This info will be written to stdout, as well as passed
                        as cmd arguments to the analysis tool in case of
                        --analysis_tool_path was specified. (Print nothing if
                        you don't need any runtime info.) 3. Print "-----end
                        workload info-----". 4. Print "Ready to trace. Press
                        enter to continue" when you wish the tracing to start.
                        5. Wait until enter is pressed, and only then start
                        executing the code you wish to run while tracing. 6.
                        Print "Stop tracing" when you wish the tracing to
                        stop. (If any of the messages isn't printed, it will
                        probably seem like memory_tracer.py is stuck.)
  --print_trace_info    If specified, memory_tracer.py would also print some
                        additional trace info:
                        num_of_events_waiting_in_trace_buf (only if it isn't
                        0, which probably shouldn't happen);
                        num_of_GMBE_events_since_enabling_GMBEOO (excluding
                        non-CPL3 GMBE events, in case
                        --trace_only_CPL3_code_GMBE was specified);
                        num_of_events_written_to_trace_buf;
                        num_of_missing_events (i.e.
                        `num_of_events_written_to_trace_buf -
                        num_of_events_written_to_trace_file -
                        num_of_events_waiting_in_trace_buf`, but only if it
                        isn't 0, which is probably a bug in qemu_with_GMBEOO);
                        actual_tracing_ratio (i.e.
                        num_of_GMBE_events_since_enabling_GMBEOO /
                        num_of_events_written_to_trace_buf);
                        num_of_dropped_events (i.e. events such that when
                        qemu_with_GMBEOO tried to write them to the trace_buf,
                        it was full, so they were discarded. This shouldn't
                        happen normally.
  --dont_exit_qemu_when_done
                        If specified, qemu won't be terminated after running
                        the workload, and you would be able to use the
                        terminal to send monitor commands, as well as use the
                        qemu guest directly, in case you have a graphic
                        interface (which isn't the case if you are running
                        memory_tracer.py on a remote server using ssh). Still,
                        you would be able to use the qemu guest, e.g. by
                        connecting to it using ssh. Remember that the guest
                        would probably be in the state it was before running
                        the workload, which is probably a quite uncommon
                        state, e.g. /dev/tty is overwritten by /dev/ttyS0.
  --verbose, -v         If specified, debug messages are printed.



future work (i.e. ideas for ways to improve memory_tracer):
1. Squeeze `GMBEOO_TraceRecord` into 8 bytes (the current implementation of
   qemu_with_GMBEOO assumes that `sizeof(GMBEOO_TraceRecord)` is a power of 2,
   and it is currently 16 bytes). To achieve this, you can use some of the
   highest bits in a 64bit virtual address (as they are always equal to the
   most significant bit (see
   https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt)) to store
   the CPL of the code that made the memory access, as well as whether it was
   a load/store, etc.
2. With regard to optimizing the code that handles a GMBE event, it might be
   better to not extract the CPL in the handler of a GMBE event, and instead
   extract it in the handler of an event that occurs on every CPL change (and
   just store the current CPL in a global variable).
3. Lluis Vilanova (who is also a PostDoc in the Technion) has done a lot of
   work on qemu. Some of his work wasn't merged into upstream qemu, and he said
   that we might leverage this work to significantly improve the performance of
   memory_tracer (though it would require a non-trivial amount of work). Lluis'
   work on qemu can be found (if I understand correctly) at
   https://projects.gso.ac.upc.edu/projects/qemu-dbi.
4. qemu_with_GMBEOO was forked from qemu tag v3.0.0 (August 2018). I guess
   the performance of upstream qemu would improve in the future, so merging
   upstream into qemu_with_GMBEOO would probably help.
   Note that merging upstream qemu into qemu_with_GMBEOO might break
   `GMBEOO_add_cpl_to_GMBE_info_if_should_trace` (it is quite obvious from the
   code of this short function what might break it, and how to easily fix that).
5. With regard to optimizing `writeout_thread` (in qemu_with_GMBEOO/trace/simple.c),
   you might use a bitmap of `TRACE_BUF_LEN / sizeof(GMBEOO_TraceRecord)` bits
   to store for each trace record in trace_buf whether it is valid or not. Then,
   you might also use x86's bit scan instructions to find the first invalid
   trace record.

